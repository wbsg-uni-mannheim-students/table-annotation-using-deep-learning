{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64cedddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adapted from DODUO: https://github.com/megagonlabs/doduo\n",
    "#Define which GPU to use\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089853bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90574d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import (\n",
    "    collate_fn,\n",
    "    #CTA\n",
    "    CTASingleColumnDataset,\n",
    "    CTAAllTableDataset,\n",
    "    #CPA\n",
    "    CPASingleColumnDataset,\n",
    "    CPAAllTableDataset,\n",
    ")\n",
    "from model import BertForMultiOutputClassification, BertMultiPairPooler\n",
    "from util import f1_score_multilabel, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d62ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe0f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the method to evaluate\n",
    "#Possible method: cta, cpa or doduo\n",
    "method = 'doduo'\n",
    "#Specify which serialization strategy to use\n",
    "#Possible: single-column, all-table ... (+ new ones)\n",
    "serialization = 'single-column'\n",
    "#Specify which language model to use:\n",
    "#bert-base-uncased, roberta-base ...\n",
    "model_name = 'bert-base-uncased'\n",
    "#Doduo can be used only with BERT and all-table serialization\n",
    "#RoBERTa can not be used with all-table serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6618afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 'cta':\n",
    "    tasks = ['cta']\n",
    "elif method == 'cpa':\n",
    "    tasks = ['cpa']\n",
    "else: #doduo\n",
    "    tasks = ['cta', 'cpa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09eb71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "max_length = 32\n",
    "batch_size = 16\n",
    "num_train_epochs = 5\n",
    "#Number of classes per task\n",
    "task_num_class_dict = {\n",
    "        \"cta\": 91,\n",
    "        \"cpa\": 176\n",
    "    }\n",
    "filepaths_task_dict = {\n",
    "    \"cta\": \"data/CTA/cta_lm.pkl\",\n",
    "    \"cpa\": \"data/CPA/cpa_lm.pkl\",\n",
    "}\n",
    "serialization_method_dict = {\n",
    "    \"cta\": {\n",
    "        \"single-column\": CTASingleColumnDataset,\n",
    "        \"all-table\": CTAAllTableDataset\n",
    "    },\n",
    "    \"cpa\": {\n",
    "        \"single-column\": CPASingleColumnDataset,\n",
    "        \"all-table\": CPAAllTableDataset\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71b3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model/'):\n",
    "    print(\"{} not exists. Created\".format(dirpath))\n",
    "    os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d95a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer based on language model\n",
    "if 'roberta' in model_name:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    base_model = 'roberta'\n",
    "else:\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    base_model = 'bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ff7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If Doduo there are two models: one for CTA and one for CPA\n",
    "models = []\n",
    "train_datasets = []\n",
    "train_dataloaders = []\n",
    "valid_datasets = []\n",
    "valid_dataloaders = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "005c3e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiOutputClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultiOutputClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiOutputClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiOutputClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading already processed train dataset\n",
      "Loading already processed dev dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiOutputClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultiOutputClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiOutputClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiOutputClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use column-pair pooling\n",
      "Loading already processed train dataset\n",
      "Loading already processed dev dataset\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    if serialization == 'single-column':\n",
    "        #Choose model\n",
    "        if 'roberta' in model_name:\n",
    "            model_config = RobertaConfig.from_pretrained(model_name, num_labels=task_num_class_dict[task])\n",
    "            model = RobertaForSequenceClassification(model_config)\n",
    "        else:\n",
    "            model_config = BertConfig.from_pretrained(model_name, num_labels=task_num_class_dict[task])\n",
    "            model = BertForSequenceClassification(model_config)\n",
    "        \n",
    "        #Choose serialization\n",
    "        dataset_serialization = serialization_method_dict[task][serialization]\n",
    "        \n",
    "    #Add more conditions when adding new serialization methods        \n",
    "    else:\n",
    "        if 'roberta' in model_name:\n",
    "            model = RobertaForMultiOutputClassification.from_pretrained(\n",
    "                    model_name,\n",
    "                    num_labels=task_num_class_dict[task],\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "                )\n",
    "        else:\n",
    "            model = BertForMultiOutputClassification.from_pretrained(\n",
    "                    model_name,\n",
    "                    num_labels=task_num_class_dict[task],\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "                )\n",
    "        \n",
    "        dataset_serialization = serialization_method_dict[task][serialization]\n",
    "            \n",
    "        #Doduo:\n",
    "        if task == \"cpa\":\n",
    "            # Use column pair embeddings\n",
    "            config = BertConfig.from_pretrained(model_name)\n",
    "            model.bert.pooler = BertMultiPairPooler(config).to(device)\n",
    "            \n",
    "            \n",
    "    #Load training and validation datasets and datasetloaders\n",
    "    train_dataset = dataset_serialization(filepath=filepaths_task_dict[task],\n",
    "                                    split=\"train\",\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    max_length=max_length,\n",
    "                                    bert=base_model,\n",
    "                                    device=device)\n",
    "\n",
    "    valid_dataset = dataset_serialization(filepath=filepaths_task_dict[task],\n",
    "                                split=\"dev\",\n",
    "                                tokenizer=tokenizer,\n",
    "                                max_length=max_length,\n",
    "                                bert=base_model,\n",
    "                                device=device)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "    valid_dataloader = DataLoader(valid_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    train_datasets.append(train_dataset)\n",
    "    train_dataloaders.append(train_dataloader)\n",
    "    valid_datasets.append(valid_dataset)\n",
    "    valid_dataloaders.append(valid_dataloader)\n",
    "\n",
    "    models.append(model.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4722f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = []\n",
    "schedulers = []\n",
    "loss_fns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad2b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, train_dataloader in enumerate(train_dataloaders):\n",
    "    t_total = len(train_dataloader) * num_train_epochs\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in models[i].named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\":\n",
    "            0.0\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in models[i].named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\":\n",
    "            0.0\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=t_total)\n",
    "    optimizers.append(optimizer)\n",
    "    schedulers.append(scheduler)\n",
    "    loss_fns.append(CrossEntropyLoss())\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6dd3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vl_micro_f1s = [-1 for _ in range(len(tasks))]\n",
    "best_vl_macro_f1s = [-1 for _ in range(len(tasks))]\n",
    "epoch_evaluation_results = [[] for _ in range(len(tasks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec5f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f7ae98c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ceph/kkorini/team-project/util.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "  class_r = conf_mat[:, 1, 1] /  conf_mat[:, :, 1].sum(axis=1)\n",
      "/ceph/kkorini/team-project/util.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "  class_r = conf_mat[:, 1, 1] /  conf_mat[:, :, 1].sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (cta): tr_loss=0.9917576 tr_macro_f1=0.6761 tr_micro_f1=0.7589  vl_loss=0.5147462 vl_macro_f1=0.8144 vl_micro_f1=0.8658 (Total time: 636.12 sec.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ceph/kkorini/team-project/util.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "  class_r = conf_mat[:, 1, 1] /  conf_mat[:, :, 1].sum(axis=1)\n",
      "/ceph/kkorini/team-project/util.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "  class_r = conf_mat[:, 1, 1] /  conf_mat[:, :, 1].sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (cpa): tr_loss=1.0363383 tr_macro_f1=0.5251 tr_micro_f1=0.6425  vl_loss=0.5396246 vl_macro_f1=0.7158 vl_micro_f1=0.7984 (Total time: 680.11 sec.)\n",
      "Epoch 1 (cta): tr_loss=0.3535146 tr_macro_f1=0.8671 tr_micro_f1=0.8973  vl_loss=0.4094755 vl_macro_f1=0.8612 vl_micro_f1=0.8889 (Total time: 618.50 sec.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ceph/kkorini/team-project/util.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "  class_r = conf_mat[:, 1, 1] /  conf_mat[:, :, 1].sum(axis=1)\n",
      "/ceph/kkorini/team-project/util.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "  class_r = conf_mat[:, 1, 1] /  conf_mat[:, :, 1].sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (cpa): tr_loss=0.3507748 tr_macro_f1=0.7985 tr_micro_f1=0.8532  vl_loss=0.4035558 vl_macro_f1=0.7863 vl_micro_f1=0.8393 (Total time: 675.62 sec.)\n",
      "Epoch 2 (cta): tr_loss=0.2214538 tr_macro_f1=0.9174 tr_micro_f1=0.9319  vl_loss=0.4172381 vl_macro_f1=0.8654 vl_micro_f1=0.8907 (Total time: 613.47 sec.)\n",
      "Epoch 2 (cpa): tr_loss=0.2314727 tr_macro_f1=0.8666 tr_micro_f1=0.8975  vl_loss=0.3682750 vl_macro_f1=0.8087 vl_micro_f1=0.8517 (Total time: 673.11 sec.)\n",
      "Epoch 3 (cta): tr_loss=0.1418599 tr_macro_f1=0.9483 tr_micro_f1=0.9541  vl_loss=0.3939214 vl_macro_f1=0.8793 vl_micro_f1=0.9014 (Total time: 613.92 sec.)\n",
      "Epoch 3 (cpa): tr_loss=0.1623803 tr_macro_f1=0.9099 tr_micro_f1=0.9269  vl_loss=0.3622168 vl_macro_f1=0.8196 vl_micro_f1=0.8603 (Total time: 678.29 sec.)\n",
      "Epoch 4 (cta): tr_loss=0.0885947 tr_macro_f1=0.9710 tr_micro_f1=0.9725  vl_loss=0.4003124 vl_macro_f1=0.8816 vl_micro_f1=0.9052 (Total time: 617.00 sec.)\n",
      "Epoch 4 (cpa): tr_loss=0.1154948 tr_macro_f1=0.9403 tr_micro_f1=0.9500  vl_loss=0.3686158 vl_macro_f1=0.8227 vl_micro_f1=0.8622 (Total time: 674.93 sec.)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "    for task_number, (task, model, train_dataset, valid_dataset, train_dataloader,\n",
    "            valid_dataloader, optimizer, scheduler, loss_fn,\n",
    "            epoch_evaluation_result) in enumerate(\n",
    "                zip(tasks, models, train_datasets, valid_datasets,\n",
    "                    train_dataloaders, valid_dataloaders, optimizers,\n",
    "                    schedulers, loss_fns, epoch_evaluation_results)):\n",
    "        t1 = time()\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        training_loss = 0.\n",
    "        training_predictions = []\n",
    "        training_labels = []\n",
    "        \n",
    "        validation_loss = 0.\n",
    "        validation_predictions = []\n",
    "        validation_labels = []\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            if serialization == 'single-column':\n",
    "                \n",
    "                #Retrive input ids, attention masks and labels for batch\n",
    "                batch_input_ids = batch[\"data\"].T.to(device)\n",
    "                batch_mask = batch[\"attention\"].T.to(device)\n",
    "                #For cross-entropy loss labels should not be vectors\n",
    "                batch_labels = torch.tensor([label.tolist().index(1) for label in batch[\"label\"]]).to(device)\n",
    "                loss, logits = model(batch_input_ids, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels, return_dict=False)\n",
    "\n",
    "                #Retrive predicted labels for batch\n",
    "                for pred in logits.argmax(axis=-1):\n",
    "                    y = [0] * logits.shape[1]\n",
    "                    y[pred] = 1\n",
    "                    training_predictions.append(y)\n",
    "\n",
    "                #True labels:\n",
    "                training_labels += batch[\"label\"].cpu().detach().numpy().tolist()\n",
    "                \n",
    "            else:\n",
    "                # Table serialization case\n",
    "                batch_labels = torch.tensor([label.tolist().index(1) for label in batch[\"label\"] if 1 in label.tolist()]).to(device)\n",
    "                logits, = model(input_ids = batch[\"data\"].T)\n",
    "\n",
    "                # Align the tensor shape when the size is 1\n",
    "                if len(logits.shape) == 2:\n",
    "                    logits = logits.unsqueeze(0)\n",
    "                \n",
    "                cls_indexes = torch.nonzero( batch[\"data\"].T == tokenizer.cls_token_id)\n",
    "                filtered_logits = torch.zeros(cls_indexes.shape[0], logits.shape[2]).to(device)\n",
    "                \n",
    "                #Mark where CLS tokens are located\n",
    "                for n in range(cls_indexes.shape[0]):\n",
    "                    i, j = cls_indexes[n]\n",
    "                    logit_n = logits[i, j, :]\n",
    "                    filtered_logits[n] = logit_n\n",
    "                    \n",
    "                if task == 'cta':\n",
    "                    for pred in filtered_logits.argmax(axis=-1):\n",
    "                        y = [0] * filtered_logits.shape[1]\n",
    "                        y[pred] = 1\n",
    "                        training_predictions.append(y)\n",
    "                        \n",
    "                    training_labels += batch[\"label\"].cpu().detach().numpy().tolist()\n",
    "                    loss = loss_fn(filtered_logits, batch_labels)\n",
    "                    \n",
    "                else:\n",
    "                    #Change\n",
    "                    all_preds = []\n",
    "                    for pred in filtered_logits.argmax(axis=-1):\n",
    "                        y = [0] * filtered_logits.shape[1]\n",
    "                        y[pred] = 1\n",
    "                        all_preds.append(y)\n",
    "                    \n",
    "                    all_labels = batch[\"label\"].cpu().detach().numpy()\n",
    "                    # Ignore the very first CLS token\n",
    "                    idxes = np.where(all_labels > 0)[0]\n",
    "                    #set_trace()\n",
    "                \n",
    "                    all_preds_filtered = [ pred for i, pred in enumerate(all_preds) if i in idxes ]\n",
    "                    all_labels_filtered = [label.tolist() for label in batch[\"label\"] if 1 in label.tolist()]\n",
    "                    \n",
    "                    training_predictions += all_preds_filtered\n",
    "                    #set_trace()\n",
    "                    training_labels += all_labels_filtered  \n",
    "                    loss = loss_fn(filtered_logits, batch[\"label\"].float())\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            # Accumulate the training loss over all of the batches\n",
    "            training_loss += loss.item()\n",
    "            \n",
    "            #Update parameters and take a step using the calculated gradient\n",
    "            optimizer.step()\n",
    "            #Update learning rate\n",
    "            scheduler.step()\n",
    "            #Clear previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "        training_loss /= (len(train_dataset) / batch_size)\n",
    "\n",
    "        tr_micro_f1, tr_macro_f1, tr_class_f1, _ = f1_score_multilabel(training_labels, training_predictions)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        for batch_idx, batch in enumerate(valid_dataloader):\n",
    "            if serialization == 'single-column':\n",
    "                                \n",
    "                batch_input_ids = batch[\"data\"].T.to(device)\n",
    "                batch_mask = batch[\"attention\"].T.to(device)\n",
    "                #For cross-entropy loss labels should not be vectors\n",
    "                batch_labels = torch.tensor([label.tolist().index(1) for label in batch[\"label\"]]).to(device)\n",
    "\n",
    "                loss, logits = model(batch_input_ids, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels, return_dict=False)\n",
    "\n",
    "                for p in logits.argmax(axis=-1):\n",
    "                    y = [0] * logits.shape[1]\n",
    "                    y[p] = 1\n",
    "                    validation_predictions.append(y)\n",
    "\n",
    "                validation_labels += batch[\"label\"].cpu().detach().numpy().tolist()\n",
    "                \n",
    "            else:\n",
    "                batch_labels = torch.tensor([label.tolist().index(1) for label in batch[\"label\"] if 1 in label.tolist()]).to(device)\n",
    "                logits, = model(input_ids = batch[\"data\"].T)\n",
    "\n",
    "                # Align the tensor shape when the size is 1\n",
    "                if len(logits.shape) == 2:\n",
    "                    logits = logits.unsqueeze(0)\n",
    "                \n",
    "                cls_indexes = torch.nonzero( batch[\"data\"].T == tokenizer.cls_token_id)\n",
    "                filtered_logits = torch.zeros(cls_indexes.shape[0], logits.shape[2]).to(device)\n",
    "                \n",
    "                #Mark where CLS tokens are located\n",
    "                for n in range(cls_indexes.shape[0]):\n",
    "                    i, j = cls_indexes[n]\n",
    "                    logit_n = logits[i, j, :]\n",
    "                    filtered_logits[n] = logit_n\n",
    "                    \n",
    "                if task == 'cta':\n",
    "                    for pred in filtered_logits.argmax(axis=-1):\n",
    "                        y = [0] * filtered_logits.shape[1]\n",
    "                        y[pred] = 1\n",
    "                        validation_predictions.append(y)\n",
    "                        \n",
    "                    validation_labels += batch[\"label\"].cpu().detach().numpy().tolist()\n",
    "                    loss = loss_fn(filtered_logits, batch_labels)\n",
    "                    \n",
    "                else:\n",
    "                    all_preds = []\n",
    "                    for pred in filtered_logits.argmax(axis=-1):\n",
    "                        y = [0] * filtered_logits.shape[1]\n",
    "                        y[pred] = 1\n",
    "                        all_preds.append(y)\n",
    "\n",
    "                    all_labels = batch[\"label\"].cpu().detach().numpy()\n",
    "                    # Ignore the very first CLS token\n",
    "                    idxes = np.where(all_labels > 0)[0]\n",
    "                \n",
    "                    all_preds_filtered = [ pred for i, pred in enumerate(all_preds) if i in idxes ]\n",
    "                    all_labels_filtered = [label.tolist() for label in batch[\"label\"] if 1 in label.tolist()]\n",
    "                    validation_predictions += all_preds_filtered\n",
    "                    \n",
    "                    validation_labels += all_labels_filtered \n",
    "                    loss = loss_fn(filtered_logits, batch[\"label\"].float())\n",
    "                \n",
    "            validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= (len(valid_dataset) / batch_size)\n",
    "\n",
    "        vl_micro_f1, vl_macro_f1, vl_class_f1, _ = f1_score_multilabel(validation_labels, validation_predictions)\n",
    "        \n",
    "        #Mark highest micro-F1 and save model if it outputs highest F1\n",
    "        if vl_micro_f1 > best_vl_micro_f1s[task_number]:\n",
    "            \n",
    "            best_vl_micro_f1s[task_number] = vl_micro_f1\n",
    "            model_savepath = \"model/{}_{}_{}_{}-bs{}-ml-{}.pt\".format(method, task, serialization, model_name, batch_size, max_length)    \n",
    "            torch.save(model.state_dict(), model_savepath)\n",
    "            \n",
    "        epoch_evaluation_result.append([training_loss, tr_macro_f1, tr_micro_f1, validation_loss, vl_macro_f1, vl_micro_f1])\n",
    "            \n",
    "        t2 = time()\n",
    "            \n",
    "        print(\n",
    "        \"Epoch {} ({}): tr_loss={:.7f} tr_macro_f1={:.4f} tr_micro_f1={:.4f} \"\n",
    "        .format(epoch, task, training_loss, tr_macro_f1, tr_micro_f1),\n",
    "        \"vl_loss={:.7f} vl_macro_f1={:.4f} vl_micro_f1={:.4f} (Total time: {:.2f} sec.)\"\n",
    "        .format(validation_loss, vl_macro_f1, vl_micro_f1, (t2 - t1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7b458e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task, epoch_evaluation in zip(tasks, epoch_evaluation_results):\n",
    "    loss_info_df = pd.DataFrame(epoch_evaluation,\n",
    "                                columns=[\n",
    "                                    \"tr_loss\", \"tr_f1_macro_f1\",\n",
    "                                    \"tr_f1_micro_f1\", \"vl_loss\",\n",
    "                                    \"vl_f1_macro_f1\", \"vl_f1_micro_f1\"\n",
    "                                ])\n",
    "    \n",
    "    loss_info_df.to_csv(\"model/{}_{}_{}_{}-bs{}-ml-{}_info.csv\".format(method, task, serialization, model_name, batch_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5a848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
